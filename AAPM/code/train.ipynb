{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb19c822-9634-4e01-88a1-5fc104f744a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from typing import Any, Optional, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import time\n",
    "def divide_infor_label(data):\n",
    "    link_label = data[:, 0]\n",
    "  \n",
    "    infor = data[:, 1:]\n",
    "    return link_label, infor\n",
    "\n",
    "def divide_network_edge(data):\n",
    "    network_label = data[:, :, 0]\n",
    "    sample_label=data[:, :, 1]\n",
    "    edge = data[:, :, 2:]\n",
    "    return network_label,sample_label,edge\n",
    "\n",
    "\n",
    "def get_train_test(train_data, test_data, valid_data, batch_size=64):\n",
    "    # Preprocessing\n",
    "    train = pd.read_csv(train_data, header=None, sep=',')\n",
    "    test = pd.read_csv(test_data, header=None, sep=',')\n",
    "    valid = pd.read_csv(valid_data, header=None, sep=',')\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    valid = np.array(valid)\n",
    "\n",
    "    train_link_label, train = divide_infor_label(train)\n",
    "    test_link_label, test = divide_infor_label(test)\n",
    "    valid_link_label, valid = divide_infor_label(valid)\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=0, replacement=True)  \n",
    "    train, train_link_label = rus.fit_resample(train, train_link_label)  \n",
    "\n",
    "    train = torch.from_numpy(train).unsqueeze(dim=1).float()   \n",
    "    train_link_label = np.array(train_link_label)  \n",
    "    train_link_label = torch.from_numpy(train_link_label).unsqueeze(dim=1).long()\n",
    "    train_set = TensorDataset(train, train_link_label)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)  \n",
    "\n",
    "    test = torch.from_numpy(test).unsqueeze(dim=1).float() \n",
    "    test_link_label = np.array(test_link_label)  \n",
    "    test_link_label = torch.from_numpy(test_link_label).unsqueeze(dim=1).long()\n",
    "    test_set = TensorDataset(test, test_link_label)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    valid = torch.from_numpy(valid).unsqueeze(dim=1).float() \n",
    "    valid_link_label = np.array(valid_link_label)  \n",
    "    valid_link_label = torch.from_numpy(valid_link_label).unsqueeze(dim=1).long()\n",
    "    valid_set = TensorDataset(valid, valid_link_label)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #unique, counts = np.unique(train_link_label, return_counts=True)\n",
    "    #print(\"Train labels distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "    #unique, counts = np.unique(test_link_label, return_counts=True)\n",
    "    #print(\"Test labels distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "    unique, counts = np.unique(valid_link_label, return_counts=True)\n",
    "    print(\"Valid labels distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "    return train_loader, test_loader, valid_loader\n",
    "class GradReverse(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, input: torch.Tensor, coeff: Optional[float] = 1.) -> torch.Tensor:\n",
    "        ctx.coeff = coeff\n",
    "        output = input * 1.0\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_output: torch.Tensor) -> Tuple[torch.Tensor, Any]:\n",
    "        return grad_output.neg() * ctx.coeff, None\n",
    "\n",
    "def grad_reverse(x, coeff):\n",
    "    return GradReverse.apply(x, coeff)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class Adversarial(nn.Module):\n",
    "    def __init__(self, in_dim, network_numbers):\n",
    "        super(Adversarial, self).__init__()\n",
    "\n",
    "        # Add extra convolutional layers\n",
    "        self.generality_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.target_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.sample_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.Softmax(dim=1))\n",
    "        \n",
    "        self.weight_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim+2, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.weight_softmax = nn.Sequential(\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "        # Add extra linear layers\n",
    "        self.link_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "        self.network_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, network_numbers),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "        self.residual1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.residual2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_embbing, weight_input, coeff=10):\n",
    "        edge_embbing = edge_embbing.permute(0, 2, 1)\n",
    "\n",
    "        generality_feature = self.generality_conv(edge_embbing)\n",
    "        generality_feature = self.residual1(generality_feature) + generality_feature  # Add residual connection\n",
    "        \n",
    "        generality_feature = generality_feature.view(generality_feature.size(0), -1)\n",
    "\n",
    "        target_feature = self.target_conv(edge_embbing)\n",
    "        target_feature = self.residual2(target_feature) + target_feature  # Add residual connection\n",
    "       \n",
    "        target_feature = target_feature.view(target_feature.size(0), -1)\n",
    "\n",
    "        weight_input = weight_input.permute(0, 2, 1)\n",
    "        weight_out = self.weight_conv(weight_input)\n",
    "        weight_out = weight_out.view(weight_out.size(0), -1)\n",
    "        weight_out = self.weight_softmax(weight_out)\n",
    "\n",
    "        feature = torch.zeros_like(target_feature)\n",
    "        for i in range(feature.shape[0]):\n",
    "            feature[i] = generality_feature[i] * weight_out[i][0] + target_feature[i] * weight_out[i][1]\n",
    "\n",
    "        link_output = self.link_classifier(feature)\n",
    "        sample_output = self.sample_classifier(feature)\n",
    "        reverse_feature = grad_reverse(feature, coeff)\n",
    "        network_output = self.network_classifier(reverse_feature)\n",
    "        return link_output, network_output,sample_output\n",
    "\n",
    "def get_pred(out):\n",
    "    out = out.argmax(dim=1)  # 取出每行的最大值索引\n",
    "    one = torch.ones_like(out)\n",
    "    zero = torch.zeros_like(out)\n",
    "    out = torch.where(out == 1, one, zero)  # 将最大值索引为1的位置置为1,其余置为0\n",
    "    return out \n",
    "def get_acc(out, label):\n",
    "    out = get_pred(out)\n",
    "    accuracy = (out == label).float().mean()\n",
    "    return accuracy\n",
    "def compute_gradient_penalty(model, edge, infor):\n",
    "    edge.requires_grad_(True)\n",
    "    infor.requires_grad_(True)\n",
    "\n",
    "    link_out, network_out = model(edge, infor)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=link_out,\n",
    "                                     inputs=edge,\n",
    "                                     grad_outputs=torch.ones(link_out.size()).cuda() if torch.cuda.is_available() else torch.ones(link_out.size()),\n",
    "                                     create_graph=True,\n",
    "                                     retain_graph=True,\n",
    "                                     only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "def train_Adversarial_Model(dataset, train_loader, valid_loader, model, criterion):\n",
    "\n",
    "    model_path = 'output/' + dataset + '_model/'\n",
    "    if os.path.exists(model_path):  # 清除之前运行代码生成的模型\n",
    "        shutil.rmtree(model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "\n",
    "  \n",
    "    best_valid_dir = ''\n",
    "    best_valid_auc = 0\n",
    "    \n",
    "\n",
    "    total_start_time = time.time()\n",
    "    for epoch in range(epochs+1):\n",
    "        p = epoch / epochs\n",
    "        learning_rate = initial_learning_rate / pow((1 + 10 * p), 0.75)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        loss_vec = []\n",
    "        auc_vec = []\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            infor, link_label = data\n",
    "            network_label, sample_label,edge = divide_network_edge(infor)\n",
    "            if not torch.cuda.is_available():\n",
    "                infor = infor.cuda()\n",
    "                edge = edge.cuda()\n",
    "                link_label = link_label.cuda()\n",
    "                network_label = network_label.cuda()\n",
    "                sample_label = sample_label.cuda()\n",
    "            infor = Variable(infor)\n",
    "            edge = Variable(edge)\n",
    "            link_label = Variable(link_label)\n",
    "            network_label = Variable(network_label)\n",
    "            sample_label = Variable(sample_label)\n",
    "            link_out, network_out, sample_out = model(edge,infor)\n",
    "            \n",
    "            link_loss = criterion(link_out, link_label.squeeze(1).long())\n",
    "            network_loss = criterion(network_out, network_label.squeeze(1).long())\n",
    "            sample_loss = criterion(sample_out, sample_label.squeeze(1).long())\n",
    "            loss = link_loss + network_loss+sample_loss\n",
    "            loss_vec.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            auc = roc_auc_score(link_label.cpu().numpy(), link_out.detach().cpu().numpy()[:, 1])\n",
    "            auc_vec.append(auc)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        loss = np.mean(loss_vec)\n",
    "        auc = np.mean(auc_vec)\n",
    "        valid_auc_vec = []\n",
    "        for data in valid_loader:\n",
    "            infor, link_label = data\n",
    "            _,_,edge = divide_network_edge(infor)\n",
    "            if not torch.cuda.is_available():\n",
    "                with torch.no_grad():\n",
    "                    infor = Variable(infor).cuda()\n",
    "                    edge = Variable(edge).cuda()\n",
    "                    link_label = Variable(link_label).cuda()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    infor = Variable(infor)\n",
    "                    edge = Variable(edge)\n",
    "                    link_label = Variable(link_label)\n",
    "                    \n",
    "            link_out, _,_= model(edge, infor)\n",
    "            \n",
    "            link_out_np = link_out.detach().cpu().numpy()\n",
    "            link_label_np = link_label.cpu().numpy()\n",
    "            #added_fictitious_link = False\n",
    "        \n",
    "            #if len(np.unique(link_label_np)) == 1 and not added_fictitious_link:\n",
    "            #    link_label_np = np.append(link_label_np, 1)  \n",
    "            #    link_out_np = np.append(link_out_np, [[0.5, 0.7]], axis=0)  \n",
    "            #    link_label_np = np.append(link_label_np, 0)  \n",
    "            #    link_out_np = np.append(link_out_np, [[0.7, 0.5]], axis=0)\n",
    "            #    added_fictitious_link = True\n",
    "            try:\n",
    "                valid_auc = metrics.roc_auc_score(link_label_np, link_out_np[:, 1])\n",
    "                valid_auc_vec.append(auc)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            #valid_auc = roc_auc_score()\n",
    "            #valid_auc_vec.append(valid_auc)\n",
    "            \n",
    "        valid_auc = np.mean(valid_auc_vec)\n",
    "\n",
    "        if valid_auc > best_valid_auc:\n",
    "            best_valid_auc = valid_auc\n",
    "            best_valid_dir = model_path + 'model' + str(epoch) + '.pkl'\n",
    "            torch.save(model.state_dict(), best_valid_dir)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Adversarial Model Epoch: [{}/{}], learning rate:{:.6f}, train loss:{:.4f}, train auc:{:.4f}, valid auc:{:.4f}'.format(epoch, epochs, learning_rate, loss, auc, best_valid_auc))\n",
    "    total_end_time = time.time()\n",
    "\n",
    "# 计算总的训练时间\n",
    "    total_elapsed_time = total_end_time - total_start_time\n",
    "\n",
    "    print('Total training time: {:.2f} seconds'.format(total_elapsed_time))\n",
    "    return best_valid_dir\n",
    "def test_Adversarial_Model(test_loader, adversarial_model, best_valid_dir):\n",
    "\n",
    "    adversarial_model.load_state_dict(torch.load(best_valid_dir))\n",
    "    adversarial_model.eval()\n",
    "\n",
    "    acc_vec = []\n",
    "    precision_vec=[]\n",
    "    f1_vec = []\n",
    "    auc_vec = []\n",
    "    auc_vec1=[]\n",
    "    y_score = []\n",
    "    y_true = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        infor, link_label = data\n",
    "        _, _,edge = divide_network_edge(infor)\n",
    "        if not torch.cuda.is_available():\n",
    "            with torch.no_grad():\n",
    "                infor = Variable(infor).cuda()\n",
    "                edge = Variable(edge).cuda()\n",
    "                link_label = Variable(link_label).cuda()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                infor = Variable(infor)\n",
    "                edge = Variable(edge)\n",
    "                link_label = Variable(link_label)\n",
    "\n",
    "        adversarial_out, _,_= adversarial_model(edge, infor)\n",
    "        pred = get_pred(adversarial_out).cpu()\n",
    "        link_label = link_label.squeeze(1).long().cpu()\n",
    "        \n",
    "        acc = (pred == link_label).float().mean()\n",
    "        acc_vec.append(acc.detach().cpu().numpy())\n",
    "        score = adversarial_out[:, 1].cpu().detach()\n",
    "        if i == 0:\n",
    "            y_score = score.data.cpu().numpy()\n",
    "            y_true = link_label.data.cpu().numpy()\n",
    "        else:\n",
    "            y_score = np.concatenate((y_score, score.data.cpu().numpy()), axis=0)\n",
    "            y_true = np.concatenate((y_true, link_label.data.cpu().numpy()), axis=0)\n",
    "        precision = metrics.precision_score(link_label, pred, average='weighted')\n",
    "        f1 = metrics.f1_score(link_label, pred, average='weighted')\n",
    "        precision_vec.append(precision)\n",
    "        f1_vec.append(f1)\n",
    "        try:\n",
    "            auc = metrics.roc_auc_score(link_label, pred)\n",
    "            auc_vec.append(auc)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            auc1 = roc_auc_score(link_label, adversarial_out.detach().cpu().numpy()[:, 1])\n",
    "            auc_vec1.append(auc1)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    auc = np.mean(auc_vec)\n",
    "    \n",
    "    precision = np.mean(precision_vec)\n",
    "    accuracy = np.mean(acc_vec)\n",
    "    f1_score = np.mean(f1_vec)\n",
    "    auc1 = np.mean(auc_vec1)\n",
    "    return auc, precision, accuracy, f1_score, auc1                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97a5e53-4f59-4ada-91a9-a63cb5eb2fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_Adversarial_model(dataset, train_loader, test_loader,valid_loader,network_numbers):\n",
    "    adversarial_model = Adversarial(in_dim=128, network_numbers=network_numbers)\n",
    "    #if torch.cuda.is_available():\n",
    "    #    adversarial_model = adversarial_model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_valid_dir = train_Adversarial_Model(dataset, train_loader, valid_loader, adversarial_model, criterion)\n",
    "    \n",
    "    #auc, precision, acc, f1, aupr = test_Adversarial_Model(valid_loader, adversarial_model, best_valid_dir)\n",
    "    auc, precision, acc, f1, aupr = test_Adversarial_Model(valid_loader, adversarial_model, '/output/')\n",
    "    return auc, precision, acc, f1, aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64a294f-a6e6-4ba7-a096-ac7a437bb494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target layer filename: train4_sup_all.txt ---\n",
      "Auxiliary layer filename: test4_sup_all.txt ---\n",
      "Valid labels distribution: {0: 149751, 1: 430}\n",
      "Adversarial Model Epoch: [0/5], learning rate:0.000500, train loss:3.1894, train auc:0.9277, valid auc:0.9277\n",
      "Total training time: 168.29 seconds\n",
      "Highest AUC: 0.8229511853504607 from model: model4.pkl\n",
      "Highest auc2: 0.7705548282504462 from model: model4.pkl\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "initial_learning_rate = 0.0005\n",
    "epochs = 200\n",
    "repeats= 10\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "outputpath = './output1/'\n",
    "if not os.path.exists(outputpath):\n",
    "    os.mkdir(outputpath)\n",
    "outfile = open('./output1/out.txt', 'w', encoding='utf-8')\n",
    "dataset = '6'\n",
    "#fold=1\n",
    "network_number=10\n",
    "#train_data = f'./GNU b/{dataset}_train_fold_{fold}_with_labels.txt'\n",
    "#test_data = f'./GNU b/{dataset}_test_fold_{fold}_with_labels.txt'\n",
    "#train_data = f'./disneg_data/train2_004_disneg.txt'\n",
    "#test_data = f'./disneg_data/test2_004_disneg.txt'\n",
    "#train_data = f'./newdata/train3_004.txt'\n",
    "#test_data = f'./newdata/test3_004.txt'\n",
    "train_data = f'train4_sup_all.txt'\n",
    "test_data = f'valid4_sup_all.txt'\n",
    "valid_data=f'test4_sup_all.txt'\n",
    "print('Target layer filename:', train_data, '---')\n",
    "print('Auxiliary layer filename:', test_data, '---')\n",
    "acc_t = []\n",
    "precision_t = []\n",
    "recall_t = []\n",
    "f1_t = []\n",
    "auc_t = []\n",
    "aupr_t = []    \n",
    "for repeat in range(repeats):\n",
    "    train_loader, test_loader, valid_loader = get_train_test(train_data,valid_data,test_data,64)\n",
    "    #auc, precision, acc, f1, aupr = run_Adversarial_model(dataset, train_loader, test_loader,valid_loader,network_number)\n",
    "    adversarial_model = Adversarial(in_dim=128, network_numbers=network_number)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_valid_dir = train_Adversarial_Model(dataset, train_loader, valid_loader, adversarial_model, criterion)\n",
    "    model_directory = 'output/6_model'\n",
    "\n",
    "    model_files = os.listdir(model_directory)\n",
    "    aucs = []\n",
    "    precisions = []\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    auprs = []\n",
    "\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(model_directory, model_file)\n",
    "        auc, precision, acc, f1, aupr = test_Adversarial_Model(test_loader, adversarial_model, model_path)\n",
    "        aucs.append(auc)\n",
    "        precisions.append(precision)\n",
    "        accs.append(acc)\n",
    "        f1s.append(f1)\n",
    "        auprs.append(aupr)\n",
    "    max_auc_index = aucs.index(max(aucs))\n",
    "    print(f\"Highest AUC: {aucs[max_auc_index]} from model: {model_files[max_auc_index]}\")\n",
    "    max_aupr_index = auprs.index(max(auprs))\n",
    "    print(f\"Highest auc2: {auprs[max_aupr_index]} from model: {model_files[max_aupr_index]}\")\n",
    "    #acc_t.append(acc)\n",
    "    #precision_t.append(precision)\n",
    "    #f1_t.append(f1)\n",
    "    #auc_t.append(auc)\n",
    "    #aupr_t.append(aupr) \n",
    "    #write_infor = 'repeat:{}, ROC-AUC:{:.4f}, Precision:{:.4f}, Accuracy:{:.4f}, F1_score:{:.4f}, AUPR:{:.4f}\\n'.format(\n",
    "    #    repeat + 1, acc, precision, f1, auc, aupr)\n",
    "    #print(write_infor)\n",
    "    #outfile.write(write_infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8eb09-3afa-4cbe-82f7-d24d99751b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
